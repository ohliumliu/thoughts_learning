\section{Motivation}
This is coolection of notes I am gathering regarding linear regression. The following is my focus:
\begin{itemize}
\item Useful math tools.
\item Useful interpretations, especially geometrical ones.
\item Useful practical considerations.
\end{itemize}
A lot of the results are from "Elements of Statistical Learninig".

\section{Problem definition}
We have $n$ experiments characterized by $\mathbf{x_{i}}$ with respective to $p$ features. For each measurement, there is single
output $y_i$. The task is to find the best linear model characterized by its parameters $\beta_i$ where $i=1,\cdots,p$.

From the perspective of maximum likelyhood, we assume that the observed value is the prediction by the model plus a random noise,
\begin{equation}
y_i = \beta_0 + x_{ij}\beta_j + \epsilon_i,
\end{equation} 
where the last terms is the noise term and Einstein notation is assumed. Here, $x_{ij}$ is the value for feature $j$ in the i-th measurement. One can write down
the joint probability of observing a particular set of $y_i$ given a set of $\mathbf{x}_i$. Note that the only source of randomness is from $\epsilon$. Under the 
following conditions (sufficient but may not be necessary):
\begin{itemize}
\item Each measurement is independent
\item $\epsilon_i\sim\mathrm{i.i.d}\mathcal N(0, \sigma^2)$
\end{itemize}
the joint probability is maximized by minimizing the following residual sum of square,
\begin{equation}
RSS = \sum_{i=1}^{n} \left[y_i -( \beta_0 + x_{ij}\beta_j)\right]^2.\label{eqn:RSS}
\end{equation}

There are ways to simplify Eq.~\ref{eqn:RSS}. A common practice is to arrange $\mathbf{x_{i}}$ in a $n\times (p+1)$ matrix $X$ whose columns are $\mathbf{x_i}$'s and
the first column being 1's. In this notation, the prediction is simply $X\beta$ and
\begin{eqnarray}
RSS &=& ||Y-X\cdot\beta||^2,\\
    &=& (Y-X\cdot\beta)^T(Y-X\cdot\beta),\\
    &=& (Y-X\cdot\beta)\cdot(Y-X\cdot\beta).
\end{eqnarray}
Here, $Y$ is a column vector formed by $y_i$ $(i = 1,\cdots,n)$ and $\beta$ is a column vector formed by $\beta_i$ $(i=0, \cdots, p)$. The second to last line is in terms of
matrix and the last one is in terms of vector/tensor inner product.

\section{Estimator and interpretations}
\subsection{Geometric interpretation in sample space}
Eq.~\ref{eqn:RSS} can be understood in terms of the linear superposition of $p+1$ vectors in $\mathbb{R}^{n}$. Recall that the design matrix $X$ is $n\times(p+1)$, and there are $p+1$ column vectors each with $n$ components. $X\cdot\beta$ is nothing but a compact way to generate a linear superposition of these $p+1$ vectors with coefficients $\beta_i$. Our purpose is to find the coefficients so that the linear superposition is closest to our target vector $Y$.

When $p+1 = n$, $\beta$ can be solved by solving a linear equation $Y=X\beta$. When $p+1 > n$, there could be multiple solutions, leading to overfitting. When $p+1 < n$, there is no guarantee that we can have a solution to $Y=X\beta$. This is the case in most practical cases of linear regression, where we have fewer parameters than the number of points.

Focusing on the last case, what would be the best choice of $beta$ to recover the target vector $Y$ from a linear combination of $\mathbf{x}_i$'s. Apparently, it boils down to the definition of closeness in $\mathbb{R}^n$, and the adoption of Eq.~\ref{eqn:RSS} implies the use of $L_2$ norm (Euclidean distance).
\subsection{A quick derivation using Einstein notation}
I will briefly write down a derivation of the estimator of $\beta$. I am not going to reference results written in matrix operation. Instead, I will use Einstein notation before converting the final results to matrix notation. This is the approach I learned from fluid mechanics class. In fact, the neat equations in terms of matrix operation is proved by using Einstein notation anyway.

Eq.~\ref{eqn:RSS} is rewritten as a function of $\beta$.
\begin{equation}
RSS(\beta) = \sum_{i=1}^{n}\left[ y_i - x_{ij}\beta_j\right]^2,
\end{equation}
where $j$ runs from $0$ to $p$. For a particular $k$, the stationary condition is
\begin{eqnarray}
\frac{\partial}{\partial\beta_k}RSS(\beta) &=& 0, \\
\sum_{i=1}^{n}\left(y_i-x_{ij}\beta_j\right)x_{ik} & = & 0,\\
x_{ik}x_{ij}\beta_j &=& x_{ik}y_i, \\
(X^TX\beta)_k &=& (X^Ty)_k.\label{eqn:estimator_component}
\end{eqnarray} 
The last line was reached by noting that $X_{ij}=x_{ij}$ by our construction. Since Eq.~\ref{eqn:estimator_component} applies to $k$ different components, it is equivalent to
\begin{equation}
X^TX\beta = X^Ty,
\end{equation}
which has an obvious solution
\begin{equation}
	\beta = (X^TX)^{-1}X^Ty.\label{eqn:estimator}
\end{equation}
It is useful to check that the dimension of the right hand side of Eq.~\ref{eqn:estimator} is $(p+1)\times1$ as expected.

Eq.~\ref{eqn:estimator} also provides the starting point to estimate the covariance matrix of $\beta_i$. Note that $\beta$ is a linear transformation of $Y$, so $COV(\beta)_{ij}$ can be related to $COV(Y)$ by the following,
\begin{eqnarray}
	COV(\beta)_{ij} &=& E[(\beta_i-\bar\beta_i)(\beta_j-\bar\beta_j)],\\
		       &=&E[(H(Y-\bar Y))_i(H(Y-\bar Y))_j],\\
		       &=&E[H_{il}(Y-\bar Y)_{l}H_{jm}(Y-\bar Y)_{m}],\\
		       &=&H_{il}H_{jm}COV(Y)_{lm},\\
			&=&H_{il}COV(Y)_{lm}H^T_{mj}.
\end{eqnarray}
Since this applies to each elements of $COV(\beta)$, we have in general
\begin{equation}
	COV(\beta) = H\ COV(Y)\ H^T.
\end{equation}
In particular, $H=(X^TX)X^T$ based on Eq.~\ref{eqn:estimator}. If $COV(Y)=\sigma^2I$, the covariance matrix of $\beta$ is simply
\begin{equation}
	COV(\beta) = (X^TX)^{-1}\sigma^2.
\end{equation}

The derivation above can be easily extended to the case of regularization. For this purpose, we need to limit ourselves to the discussion of the slopes $\beta_i$ ($i=1,\cdots,p$) as in Chapter~\ref{ch:linear}. Just to reiterate the rationale, the intercept shouldn't be subjected to regularization because that is a constant shift in the model; a constant shift to all the data points leads to a change in $\beta_0$ only and shouldn't introduce more or less penalty due to regularization. The target function we are trying to minimize becomes,
\begin{equation}
	\mathcal F(\beta)=RSS(\beta)+\lambda\beta_i\beta_i.\label{eqn:costReg}
\end{equation}
The stationary condition for $\beta_k$ becomes
\begin{eqnarray}
	\frac{\partial}{\partial\beta_k}\mathcal F(\beta) &=& 0, \\
	\sum_{i=1}^{n}2\left(y_i-x_{ij}\beta_j\right)(-x_{ik}) + 2\lambda\beta_k & = & 0,\\
	(x_{ik}x_{ij}+\delta_{kj}\lambda)\beta_j &=& x_{ik}y_i, \\
	((X^TX+\lambda I)\beta)_k &=& (X^Ty)_k.
\end{eqnarray} 
The last line is again written in matrix notation
\begin{equation}
	(X^TX+\lambda I)\beta = X^TY,
\end{equation}
which solves to
\begin{equation}
	\beta = (X^TX+\lambda I)^{-1}X^TY.\label{eqn:estimatorReg}
\end{equation}
Eq.~\ref{eqn:estimatorReg} is related to the original motivation of regularization. In order to guarantee that $X^TX$ is not singular, one could add a term $\lambda I$ to make it nonsingular. This may also make the matrix inversion more stable numerically. Of course, regularization has alternative, and perhaps more fundamental motivations, which are summarized below:
\begin{itemize}
	\item Make $X^TX$ non-signular as discussed just now.
	\item Extra data points as discussed in Chapter~\ref{ch:linear}. This idea can be used in the derivation above to Eq.~\ref{eqn:estimatorReg} as well.
	\item As suggested in Chapter~\ref{ch:linear} and more formally in Eq.~\ref{eqn:costReg}, regularization can be thought of as the reminant of a prior probability. In fact, Eq.~\ref{eqn:costReg} is the log-likelyhood of having $\beta$ given the observation $Y$ and a prior of $\beta_i\sim\mathcal N(0,\lambda^{-1})$ (after ignoring the denominator).
\end{itemize}
\subsection{Geometric interpretation}
As discussed earlier, we tried to minimize the difference, in $\mathbb{R}^n$, between our target vector $Y$, and a linear combination of $\mathbf{x}_i$ or $X\beta$. In this so called column space of $X$, we are trying to minimize the Euclidean distance between $Y$ and $X\beta$. The ideal case is that $d\equiv Y-X\beta=0$ has a solution. This is equivalent to requiring $d_i=0$ or $d\cdot \hat e_i=0$ for $i=1,\cdots, n$ with $\hat e_i$'s a complete set of basis. Of course, for linear regression, this is not possible, then what compromise should be adopt? Since our only knowledge is from the observation $\mathbf{x}_i$'s, it thus makes sense to use $x_{i}$ to replace $\hat e_i$. Note that $\mathbf{x}_i$ is the $i$-th column of $X$ by definition, and thus $(\mathbf{x}_i)_j=(X)_{ji}\equiv x_{ji}$. For any $k=0,\cdots,p$,
\begin{eqnarray}
\mathbf{x_k}\cdot(Y-X\beta) &=& 0, \\
\sum_{i=1}^{n}x_{ik}(y_i-x_{il}\beta_l) &=& 0.
\end{eqnarray}
This was obtained earlier.

As an aside, one more word on the column space of $X$. $X$ can be thought of as a row vector whose components are the column vectors $\mathbf{x}_i$ ($i=0,\cdots,p$). $X\beta$ can thus be thought of as the inner product of the ``row vector'' $X$ with a column vector $\beta$. The result is a linear combination of the elements of $X$. The result should have the same dimension as the elements of $X$, or a column vector $n\times1$.
\subsection{Gram-Schmidt}


